\chapter{Related Work}

In this chapter related work is discussed. In Section \ref{sec:scope} the
scope of the thesis is defined and the methodology of finding related work is
described. Thereafter related work on the topics of \rl based schedulers,
assessing robustness of \rl methods and approaches for reducing retraining
time is discussed. Lastly, the gap of in the related work is analyzed.


\section{Scope and Methodology}\label{sec:scope}

The scope of this thesis is specifically \rl based schedulers in cloud
environments. In Section \ref{sec:rl} is explained why this thesis only
focusesses on \rl based schedulers, not schedulers based on other forms of
\ml.

Comparing different methods and models is the main task in this thesis. To
compare methods of models, a justified way of assessing methods and models is
required. It is also important that the comparison is done on the indicators
on which the performance depends. These are called the \kpis. The \kpis of \rl
based schedulers are established from literature. Assessing the robustness of
a model is also necessary for this research. By assessing robustness, a way to
select a method for reducing retraining time can be selected.


\section{Reinforcement Learning}\label{sec:rl}

Most \ml based schedulers are \rl based, and therefore this thesis focuses on
reducing retraining of \rl models. The reason most \ml based schedulers are
\rl based is because of how \rl works and differs from the other \ml
paradigms. \rl and its differences with the other paradigms are very well
explained in `the most popular artificial intelligence textbook in the
world'\footnotemark, \citeA{russell2010}. Their explanation of \rl is
summarized in the following paragraph.

\footnotetext{According to \href{\urltor}{this blog}, but also shown on the
homepage of the book (\url{http://aima.cs.berkeley.edu/}).}


\subsection{Why Reinforcement Learning for Schedulers?}

Reinforcement learning is one of the three basic paradigms in machine
learning, along with supervised learning and unsupervised learning. \rl is
different from the other two paradigms. Supervised learning and unsupervised
learning have a thing in common: the need for data. Supervised learning needs
annotated data, various inputs and desired outputs are given and the algorithm
learns a function to get as close to the wanted outputs as possible given the
inputs. With unsupervised learning a model is forced to build an internal
representation of the world by mimicking the data. The reason \rl is different
is that it does not depend on data, but rather learns from a feedback loop of
rewards or reinforcements. It typically consists of one or more agents, a set
of actions $A$ and a set of states $S$. This agent performs an action $a \in
A$ to a perform state transition. This action leads to a reward. In many
complex environments \rl is the only feasible way to train models because
there might be little data available or the environment is too complex to
model \cite{russell2010}. For the reason that \rl has no need for input data
but solely needs an environment, actions and a reward function it is widely
used in research about using \ai in resource managers. The popularity of \rl
and the nature of how \rl works is the reason this research is narrowed down
to reducing retraining of \rl schedulers.


\subsection{Reinforcement Learning Based Schedulers}

Many reinforcement learning algorithms are used in state-of-the-art \rl
based schedulers. Three different state-of-the-art \rl schedulers are
described in this section.

% Reinforcement learning algorithms can be categorized in two
% categories: model-based and model-free. Model-based \rl algorithms create a
% model of the environment, which model-free \rl algorithms do not. Both have
% advantages and disadvantages, depending on the environment.

The first state-of-the-art \rl based scheduler is DeepRM, presented in
\citeA{mao2016}. This scheduler is based on deep reinforcement learning with
policy representation via a \dnn. The algorithm learns by performing
gradient-descent on the policy parameters using the REINFORCE algorithm from
\citeA{sutton1999}. For the state representation a 2-D image is used to
capture the status of resources and jobs. The second state-of-the-art
scheduler is proposed in \citeA{zhang2020} and also trains a policy network,
but in this algorithm the network is trained using \ppo, an actor-critic
algorithm. Unlike DeepRM, the model from \citeA{zhang2020} is not hard bounded
by the instance size \cite[p.~5]{zhang2020}. Because it is not hard bounded by
the instance size it is more flexible and robust.


% - ?Graph Neural Networks.?

% - deep Q learning (free)

\section{Evaluation of Reinforcement Learning Models}

% TODO: How can we effectively assess and compare RL based methods in job
% scheduling?

Currently there is not yet a standard evaluation method for \rl models. Due to
the absence of a standard evaluation model, reproducibility often becomes more
difficult. Work is done in defining standard evaluation methods, but these
methods are not yet the industry standard. Three metrics were found. The first
evaluation is regret. Regret can only be used as a metric when an agent with
optimal policy can be defined. The difference in actions taken is the regret
per action. This is defined as the reward for the action of the optimal agent
minus the reward for the taken action. Knowing the optimal policy is not
always possible. The second evaluation metric is not used literature but
rather in practice. Some non-scientific metrics that were used to evaluate \rl
models are popular, well-defined environments like the OpenAI gym
\cite{gym2016}. A comparison of \rl models is done on their
leaderboard\footlink{github.com/openai/gym/wiki/Leaderboard}. The used metric
is the number of episodes it took to solve the problem. Lastly, proposed
methods in literature are for example a framework for evaluating \rl in
\citeA{khetarpal2018} and a novel evaluation method in \citeA{jordan2020}.


\section{Assessing Robustness of Reinforcement Learning}

If a \rl model is robust, it has the ability to cope with dynamic environments
and works well in noisy environments. The goal of this thesis is to compare
methods for reducing the retraining time of \rl based schedulers. If a model
is robust, it can handle the dynamics of the environment and will most likely
not need much retraining time. Thus, making the \rl scheduler more robust will
most likely result in less retraining time. This is why related work on robust
reinforcement learning algorithms is discussed. The comparison between
robustness of reinforcement learning models is only as descriptive as the
metric used for it. This is way selecting a well-formed method for assessing
robustness is important for this research.

Many methods for assessing robustness of reinforcement learning use some kind
of ``disturbance'' in the training environment \cite{morimoto2005}. By
changing dynamic aspects of the environment, the model learns about the
dynamics and accounts for this.


In \citeA{alnima2021} the neuron coverage, i.e. the ratio of the activated
neurons in a network, is used a measurement of robustness. This is only
applicable in \rl methods with neural networks in it, for example \dqn.


% todo: link pinto as example for open AI testing


\section{Approaches to Reduce Retraining Time}\label{sec:reduce}

Reducing retraining time of reinforcement learning models is an important
task. By doing this, reinforcement learning models get more robust
and can be used in more flexible environments. This allows using reinforcement
learning as solution for problems which were too flexible for previous
\rl models. Reducing retraining time also has a cost efficient motivation;
less retraining time means less resources-heavy training with high energy
costs. In this section the current state-of-the-art approaches are discussed.
The approaches are categorized in three categories: (1) approaches modelling
the dynamic aspect of the environment, (2) approaches using adversarial agents
and (3) approaches that reduce retraining by using earlier learned policies.
The reviewed approaches to reduce retraining time are discussed on category
basis in the following subsections.


\subsection{Modelling the Dynamic Environment}\label{sec:model}

Modelling the dynamics of the environment is a straightforward approach to
taking account of the dynamics. For example, in job schedulers in cloud
environments, this can be done by adding and removing resources during
training. It would theoretically be possible for a model to learn that a
change in resources is possible and take that in account. Numerous approaches
include information about the dynamic aspect of the environment into the
model. To successfully apply \rl models in dynamic environments,
\citeA{wiering2001} proposed to include an a-priori model of the changing
parts in the environment. In \citeA{nagayoshi2013}, a method was proposed for
detecting environmental changes on which the model can adapt. The
aforementioned proposed methods require a model of the environment or a-priori
knowledge of the changing parts of the environment. In many environments, this
would not be feasible, but it could be possible in job scheduling since the
only changing element of the environment is the number of resources. The
disadvantage of modelling the dynamic environment beforehand is that it might
not be possible due to the randomness of the changes in resources. The outage
of a resource can probably be predicted when some information on the current
state of the resource is provided, but predicting the outage of a resource is
out of the scope of this thesis.


\subsection{Adversarial Agents}\label{sec:adversarial}

Another approach for learning how to handle disturbance is by adding in a
disturbing agent \cite{morimoto2005}. This agent learns to perform the most
disturbing action as possible. The same approach is also proposed in
\citeA{pinto2017}, therein called robust adversarial reinforcement learning.
In \citeA{pinto2017} it is also stated that the gap between simulation and
real world is too large for policy-learning approaches to work well. This
might be achievable if the adversarial agent can control when specific
resources inserted or deleted. This raises a number of questions. How would
the amount of operations (insertion and deletions of resources) be selected?
This has to be controlled, otherwise the adversarial agent would score best by
deleting all resources. Will the agent work better than training with random
resource operations? Are there critical moments where the deletion of
resources is worse than other moments?

Implementing an adversarial agent can be done with Q-learning models.
\Ql is a model typically used in single agent environments and is of simple
nature. Having two Q-learning agents in an environment, one learning the main
task and the other one being the adversarial agent can improve robustness.
This technique have been used in a simple two player game \cite{littman1994}.
There are also other approaches for reducing retraining time of \ql models in
specific. These approaches include repeated update Q-learning
\cite{abdallah2016} and variants of \dqn like robust \dqn \cite{chen2018}.

Another approach is to train robustness online, i.e. during deployment of the
model \cite{fisher2019}. This approach, called Robust Student-\dqn (RS-\dqn)
deploys adversarial agents online to trick the model into unwanted states.
This approach maintains competitive performance.


\subsection{Reusing Knowledge}\label{sec:reusing}

Meta reinforcement learning is a method for for learning to quickly adapt
online to new tasks. This is done in the context of model-based reinforcement
learning. Meta reinforcement learning models use meta-learning to train a
dynamics model that can be rapidly adapted to local context
\cite{nagabandi2019}. Meta learning is described as ``Learning to learn''. A
model trains to learn during meta-training. Here, the learner learns the new
task and the meta-learner trains the learner. In \cite{schweighofer2003} it is
proposed to learn meta-parameters with stochastic gradients. It is a robust
algorithm that finds appropriate meta-parameters and controls the parameters
in a dynamic, adaptive manner. Even when retraining is needed to adapt to the
changes in environment, the parameters of the model will probably be nearly
the same as before.


\section{Gap Analysis}

As shown in this chapter, much work is done in combining job scheduling and
reinforcement learning. There is also research done in reducing the retraining
time of reinforcement learning models and making these models more robust. In
this section the gap is identified and analyzed.

In Section \ref{sec:intro} multiple \rl based schedulers were discussed. These
schedulers lack robustness: flexible environments are not handled well. As
shown in Section \ref{sec:reduce} methods for improving robustness of \rl
models exist, but are not yet integrated in \rl based schedulers. Integrating
these methods would improve robustness and reduce retraining time.

There also is not a standard collection of indicators to assess robustness of
\rl based schedulers. A standard collection of robustness indicators
simplifies comparing \rl based schedulers and method for reducing retraining
time of of the schedulers.




% gap: methods for reducing require knowledge of environment?
%
%
%
%
%
%
%
%
%
%
%
%
