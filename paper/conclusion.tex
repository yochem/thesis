\chapter{Conclusion}\label{sec:conclusion}

The cloud is a network of nodes providing resources for computation. Jobs
require a specific amount of resources to complete and needs to be scheduled
on the nodes. As a result of the growth of cloud environments, the scheduling
task is becoming more and more complex. Cloud systems are currently too
complex for humans to create a scheduler with heuristics. Now that humans are
not able to design efficient schedulers, reinforcement-learning-based
schedulers learn to schedule jobs efficiently. Although these \gls{rl}-based
schedulers can achieve high efficiency, they lack robustness. After a change
in the environment, i.e. the addition or removal of nodes, the scheduler has
to retrain. In this thesis, a method for selecting approaches to reduce this
retraining time is researched. This led to the following research question:
\begin{quote}
How to select a method for reducing the retraining time of reinforcement learning
    based job schedulers in cloud environments?
\end{quote}
Three sub-questions were formed to answer the research question. The three
sub-questions will now be answered by drawing a conclusion from the literature
and results.


\section{Answering the Sub Research Questions}

The first sub research question, ``How can we effectively assess and compare
\gls{rl}-based methods in job scheduling?'' is answered using literature. Although
the OpenAI Gym is the most popular method for evaluating reinforcement
learning algorithms, there is not yet a standard evaluation method. The
metrics are the number of episodes before solve. Some environments define
solving by receiving an average reward threshold over 100 consecutive trials,
while other environments have a clear goal that can be achieved. It can be
concluded that openAIs Gym is an effective method for assessing and comparing
\gls{rl}-based methods in job scheduling.

The second sub research question, ``What are the indicators to assess the
robustness of \gls{rl}-based schedulers?'', is also answered by literature.
Indicators to assess the robustness of \gls{rl}-based schedulers are their performance
in noisy environments with disturbance. Another measurement for
assessing this performance is by changing the dynamic aspects of the
environment. For methods using deep learning, the neural coverage is a metric
for robustness. Since deep learning is used in combination with \gls{rl} by many
schedulers, this metric can also effectively indicate the robustness of
\gls{rl}-based
schedulers. Thus, to assess the robustness of \gls{rl}-based schedulers, the performance
in noisy or dynamic environments can be used as an indicator. When the scheduler
uses deep learning, the neural coverage is also an available indicator for
robustness.

The last sub research question, ``What are the state-of-the-art approaches for
reducing retraining time?'', is answered by three types of approaches. The
first approach is modelling the dynamics in the environment. The
state-of-the-art method in this approach is \citeA{nagayoshi2013}. The second
approach is the use of adversarial agents. These agents learn to disturb the
actual agent as much as possible. The state-of-the-art research on this is
\citeA{chen2018}. The last approach is meta-learning. Meta-learning learns how
to learn, which can improve learning efficiency. The \mrlco model from
\citeA{wang2020} is the state-of-the-art meta-learn, and also used in the
experiments.


\section{Answering the Research Question}

With an answer found for the three sub-questions, the research question will
be answered. Again, the research question of this thesis is ``How to select a
method for reducing retraining time of reinforcement learning based resource
schedulers in cloud environments?''. A method for reducing the retraining time
of \gls{rl}-based schedulers can be selected on three criteria: the overall
performance of the model, the robustness of the model and the training time.
The first criterion is needed to prevent the method from deteriorating the
actual model. The overall performance should not be worse than before. The
second criterion, robustness, can reduce the frequency of retraining. If a
model is more robust it might only have to retrain on large environmental
changes, which means fewer retraining, reducing training time. The last
criterion is reducing the retraining time. A method might require as many
training sessions as first, but if it greatly improves the time it takes to
train, it would also be an improvement. As discussed in Section
\ref{sec:discussion}, in Figure \ref{fig:rewards} it can be seen that
meta-learning requires fewer iterations to achieve the same reward as the
previous training session.

To conclude, selecting a method for reducing retraining time of \gls{rl}-based
schedulers can be done on three criteria:
\begin{itemize}[noitemsep]
    \item The overall performance of the model, assessed by the performance
        in a Gym environment or comparing the received reward to other models.
    \item The robustness of the model, assessed by including disturbances in
        the environment or an adversarial agent, or by controlling the
        dynamics of the environment.
    \item The training time of the model, which can be reduced by modelling
        the environment, adding in robustness through adversarial agents and
        meta-learning.
\end{itemize}


\section{Reflecting on Contributions}

In Section \ref{contributions}, the contributions of this thesis are listed.
Contributions are reflected in this section. The first contribution,
providing a approach to select a method for assessing performance, has not
been made. There are no clear methods, for example like the $F1$-score in
classification tasks, to asses \gls{rl}. The second contribution, introducing
the importance of robustness and explaining methods for robustness, has been
done. The importance of robustness in an \gls{rl}-based scheduler has been
demonstrated and methods for integrating robustness have been discussed. The
third contribution, listing the state-of-the-art approaches for reducing
retraining time, is done in Section \ref{sec:state-of-the-art} and the results
of integrating meta-learning is shown in Section \ref{sec:experiments}. The
last contribution, an approach for selecting methods for reducing retraining
time is shown earlier in this paragraph. Looking back on the listed
contributions, they have been made in a greater or lesser extent.
