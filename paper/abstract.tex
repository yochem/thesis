\begin{abstract}
    Cloud environments are expanding rapidly, becoming the backbone of the
    internet. These cloud systems are used by almost any major website on the
    internet, and many software companies offer cloud functionality as a
    service. Cloud systems provide computational resources over the internet
    to perform computational tasks that require these resources. Job
    schedulers assign these tasks to cloud resources. Efficient scheduling is
    affected by many factors, including task tides, resource availability, and
    hardware failures. A job scheduler has to take these factors into account.
    It is tedious to design an efficient job scheduler with heuristics for
    increasingly complex large-scale systems, such as a cloud environment.
    Heuristic-based approaches have their shortcomings in these complex
    systems. Reinforcement learning
    (RL) techniques have been successfully applied to job scheduling
    in prior research. Despite being more efficient than heuristics-based
    schedulers, they lack robustness and often must be retrained due to
    changes in the environment, such as a shift in the availability of
    resources. An RL-based scheduler requires retraining on changes in the
    environment. Several methods have been suggested for including robustness
    in RL or to reduce retraining time. This thesis presents indicators for
    selecting a method for reducing the retraining time of RL-based schedulers
    in cloud environments. The indicators on which a method can be selected
    are the overall performance of the model, the robustness of the model and
    the (real) training time on the model.
\end{abstract}
