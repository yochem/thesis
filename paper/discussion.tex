\chapter{Discussion}\label{sec:discussion}

This thesis, like every other thesis, has its achievements and shortcomings.
These aspects of the thesis are discussed in this section. First, the
achievements that are gained are summed up, and after this, the shortcomings
are discussed. The obtained results are discussed and it is reflected on the
methodology.

\section{Achievements}

The first experiment has successfully shown that meta-learning improves
learning speed and reduces retraining time. In Table \ref{table:exectime}, it
is shown that the execution time of policy and environment is the same for the
first and second run, meaning that running 2000 iterations for both models has
the same duration. However, running all 2000 iterations is not necessary. The
first run baseline model achieved its maximum reward of -6.7 in iteration 1345
(Table \ref{table:max-reward}). The second run, which was meta-learning,
achieved the same reward in only 521 iterations. It can be stopped after this
iteration. If the model is stopped after achieving the maximal reward of the
baseline model, the retraining time can be reduced by 105 minutes, from 171
minutes to 66 minutes duration, a reduction of 61.4\%. The progress of the
average reward during training is plotted in Figure \ref{fig:rewards}. The
first run on Euro-Argo data does not have a tipping point, but the second run,
which has meta-learned has a tipping point after around 250 iterations, and
after only 280 iterations the reward is the same as the reward of the first
run after 1250 iterations. This shows that meta-learning decreases the number
of iterations needed for \mrlco to converge.

Meta-learning also works on new data. The model learned the second batch of
the Euro-Argo data in fewer iterations than without meta-learning. It even
learns fast despite the reward starting much lower than the reward of the
first batch. Learning quicker with different data shows robustness. The model
can work with different data and thus learns quicker to work in dynamic
environments.

Although the reward on the Euro-Argo data fluctuates less than the \mrlco
data, the return value of the Euro-Argo has a larger spread, i.e. the
difference between the maximal return value and the minimal return value, see
Figure \ref{fig:return}. A reason to explain this difference between the
\mrlco data and the Euro-Argo data is not found.

% todo: talk about second experiment?


The first achievement was the successful converint of the Euro-Argo log
data to the Taillard specification and the \DAG data. Even though the \DAG
data provided by the \mrlco model had some quirks and was not as described how
it should be, it was managed to convert the Euro-Argo data so that it works with the
\mrlco model. The conversion provided insights into the many aspects of an
instance for job scheduling. Many parameters were available to change the \DAG
data and change the job flow accordingly.


\section{Shortcomings}

During the experiments it was not managed to finish all experiments that were
planned. The results are not the expected results from the start of the
research project. Most of the shortcomings that led to not achieving the
intended results are due to the models that did not meet the expectations on
standardization and clear documentation. Converting the data for every model
that would be evaluated was not feasible in the amount of time for this
project.

Another shortcoming of the converting of data is that converting data into
other formats can also create unfairness in the evaluation. It might be that
one data format provides more information to the algorithm than other data
formats. Receiving more information means learning can possibly go faster.
Difference in data formats would make the comparison that was intended in this
research invalid.
