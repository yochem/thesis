\chapter{Implementation}\label{sec:method}

In this chapter, the implementation of the system is described. Approaches to
reduce retraining time of \gls{rl}-based schedulers are compared in this
thesis. The approaches are compared against multiple \gls{rl}-based schedulers
to evaluate how well the approaches work on reducing retraining time.
Therefor, firstly the workflow of the experiments is described. Secondly, the
software and hardware for the experiments is described (ยง\ref{sec:setup}).
Lastly the data and its usage are described in Section \ref{sec:data}.

\section{Methodology}

To address the research questions and fill the gaps described in Section
\ref{sec:gap} two experiments are composed. Related work was required for the
set up of the experiments. The methodology for discovering related work was
explained in Section \ref{sec:scope}. The state-of-the-art meta-learning model
and other state-of-the-art \gls{rl} models were selected from literature to
compare the approaches for reducing retraining time. At first, these methods were
assessed on the number of iterations it takes to converge, as done in the
OpenAI Gym. Thereafter, the methods for reducing retraining time were
incorporated and the state-of-the-art \gls{rl} models were evaluated again.
The robustness was evaluated based on the new learning time and handling
dynamics in the environment. Lastly, indicators that indicate robustness or
reducing retraining time are sought. To compare the methods, they are trained
on the same data. Because all methods expect the data to be in a different
format, the data had to be converted. The conversion of the data is described
in section \ref{sec:data}.


\section{Hardware and Software Requirements}\label{sec:setup}

This section explains the hardware and software requirements for the
implementation. Although the experiments might work in other environments, the
results will be the same if using the same requirements. Firstly the hardware
used is described, and after the hardware some vital software and its usage is
described. The experiments were run on Workstation 8 provided by the Robotics
Lab at the University of Amsterdam. Workstation 8 is an Alienware Aurora R9
1.0.4 machine, with an Intel i9-9900 (16 threads) CPU running at 5.000 GHz,
and a NVidia 2080 Ti (11 GB) GPU with CUDA 10 and driver 415. The operating
system is Ubuntu 18.04.4. All algorithms were implemented in the Python
programming language\footlink{python.org}. A virtual Python environment was
created for every experiment to prevent versioning issues between
dependencies. This also allowed us to run a different Python version per
algorithm. The tool used for creating and managing virtual Python environments
is called \code{pyenv}\footlink{github.com/pyenv/pyenv}. An example of setting
up a virtual environment:

\begin{verbatim}
$ pyenv install 3.7.10
$ pyenv virtualenv 3.7.10 meta-rl
$ pyenv activate
\end{verbatim}

The above commands install Python 3.7.10, create a virtual environment called
`meta-rl' that uses Python version 3.7.10 and activates this virtual
environment. From then on, packages could be installed using the Python
package installer \code{pip} and can only be used in the `meta-rl'
environment. Standard practice is to include a \code{requirements.txt} file in
the root directory of the project. However, only some of the projects used in
this thesis included it. Installing the needed packages at the right version
is as simple as running the command \code{pip install -r requirements.txt}.

Frequently used Python packages in scientific computing are the following:
NumPy, Scipy, Keras, Tensorflow and PyTorch\footnotemark. NumPy and SciPy
provide an API for scientific computing and a solid multidimensional array
object. The other three provide a back-end for creating well-optimised \ml
algorithms, especially for \glspl{nn}. Another software project used extensively with
this thesis is the OpenAI Gym \cite{gym2016}. Gym is a package for developing
and comparing reinforcement learning algorithms. It provides access to many
environments to develop an \gls{rl} algorithm on.

\footnotetext{Their respective home pages: \link{numpy.org}, \link{scipy.org},
\link{keras.io}, \link{tensorflow.org} and \link{pytorch.org}.}


\section{Workload Data Set}\label{sec:data}

The supervisors provided four datasets at the start of the thesis. The data is
log data from the Euro-Argo project\footlink{euro-argo.eu/}. The datasets
described received messages in intervals of one or ten minutes. The four
datasets provided 43,200, 182,283, 4,320 and 28,311 rows of data, which sums
up to 258,117 rows of data in total. The datasets all had five columns: the
start time, the transfer time, request number, byte count and completion
status. An overview of the datasets is provided in Table \ref{table:data}.

\input{./fig/table-data.tex}

The data was complete, i.e. it had zero missing values. Not having missing
values does not mean the data is clean. The dataset contained 3 rows where the
transfer time, request id and byte count had the value of 0. The 3 rows were
dropped because it leads to problems during converting the data into
another format. Another quirk to note is that even though the column
name `request ID' suggests its values will be IDs and thus unique, this
is not the case. For example, there are 1,576 unique IDs in the 4,320 rows of
the \code{mts\_june\_10m.csv} file. All counts of unique IDs and rows are
shown in Table \ref{table:files}. The commands for getting this data can be
found in Appendix \ref{app:dataset}.

\input{./fig/table-files.tex}

\subsection{Data Format Conversion}

The Euro-Argo dataset was not compatible with most models. It is not
compatible with most models because it is not a \jss dataset. The data are
incoming requests from many Internet of Things-like devices. The data is
sequential and are not different jobs that need to be scheduled. It is
possible to convert the original data to formats that are more used in \jss
tasks. In the following two sections, two conversions are discussed.


\subsubsection{Directed Acyclic Graph}\label{sec:dag}

\begin{sloppypar}

    The first format in which the data was converted was a \DAG. Following the
    example of the provided data by the model that uses \DAG data\footnote{The
    model called metaRL-offloading uses DAG data. The example data can be
    found here: \urlmetadata.}, the Euro-Argo data was converted to a \DAG
    using DAGGEN\footlink{github.com/frs69wq/daggen}. DAGGEN is a command line
    tool that generates a \DAG usable by scheduling algorithms. Because the
    Euro-Argo data was originally not \jss data but log data of requests, the
    converted data used parameters and other aspects from the example data.
    The distribution of all parameters at the example data was also used on
    the new data. For the $D_{min}$ and $D_{max}$ parameters, the transfer
    time from the Euro-Argo data is used. An example of a generated DAGGEN
    command is the following: \texttt{daggen -{}-dot -n 4 -{}-mindata 2656
    -{}-maxdata 2658 -{}-density 0.4 -{}-ccr 0.5 -{}-fat 0.6 -{}-regular 0.5
    -o graphs/4-2657.gv}. The generated graph file from this command is shown
    in Figure \ref{fig:dag}. The only two parameters that inherits their value
    from the Euro-Argo data are \code{mindata} and \code{maxdata}. As
    mentioned, the other parameters followed the same distribution as the
    example data from \gls{mrlco}. The commands for finding the distributions
    of the parameters are listed in \ref{app:mrlco}. It can be seen in Figure
    \ref{fig:dag} that the tasks on the lines without an arrow (\code{->})
    contain an attribute \code{expect\_size}. This can not be added with the
    DAGGEN tool and is probably added by the authors of the model that uses
    this data. We noticed that the expected size is 2.00, 2.50, or 3.33 times
    smaller than the size attribute of that task. The distribution was almost
    uniform: 16500 times a factor of 2, 16820 times a factor of 2.50, and
    16680 times a factor of 3.33. The expected size was thereafter integrated
    using the factors as described above.

\end{sloppypar}


\input{./fig/dag.tex}

The pseudocode for converting the Euro-Argo data to \gls{dag} data is shown
and described in Figure \ref{fig:pseudo}.

\input{./fig/pseudo.tex}

\subsubsection{Taillard Specification}\label{sec:taillard}

Other models worked with the popular OpenAI Gym environment. The Gym
environment allows people to build custom environments to make it possible to
test \gls{rl} algorithms in other kinds of environments. Gym itself provides
game-like environments. The interface for Gym is very minimal, as typical
\gls{rl}
algorithms have simple interaction with the environment. Most importantly, it
exposes the following methods and attributes: \code{env.reset()} which resets
the environment and returns the start state, \code{env.observation\_space}
which resembles the current state, \code{env.action\_space} which are all
possible actions in the current state, and \code{env.step(action)} which
performs an action and returns the new state, a reward for the action and if
the model is done.

In \citeA{tassel2021} a gym environment called \code{JSSEnv} for simulating
the job shop scheduling problem is proposed. This environment provides the
bridge between instances specified in the Taillard specification and the Gym
interface. The Taillard specification, proposed in \citeA{taillard1993}, is a
specification for computing instances. It specifies the number of machines
$\mathcal{M}$ and jobs $\mathcal{J}$ and processing time per job for all
machines. An example for an instance with one job and three machines. The
machines are numbered from 0 to 2, and the processing time of the job is
respectively 5, 8 and 9. The visiting order is 1, 2, 0. The representation of
this instance is shown in Figure \ref{fig:taillard}.

\input{./fig/taillard.tex}

As can be seen, first the number of jobs and the number of machines and then
one job per line, with the visiting order and their processing time. To apply
the Euro-Argo data to this format, we first analysed the existing instances.
In the instances provided by \citeA{tassel2021} there were either 40 or 45
machines, evenly distributed. The distribution of the number of jobs is shown
in Figure \ref{fig:jobs}. The Euro-Argo data was sequential, as is every line
in the Taillard specification. One line describes the path one job travels
between machines. To convert the Euro-Argo data into Taillards' specification
we only used the transfer time. The transfer time was converted in the
processing time in the Taillard specification. Then the data was divided in
groups of size $\mathcal{M}$, then popped $\mathcal{J}$ items from these
groups, shuffled the machine IDs ($id \in [0, \mathcal{M})$) and zipped the
shuffled machine IDs and the transfer times together into one line for the
Taillard instance.

\input{./fig/jobs.tex}
